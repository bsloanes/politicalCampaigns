{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a0lo7fiUgjO"
      },
      "outputs": [],
      "source": [
        "#One campaign, one API key, one pre-set table.\n",
        "#Must download json + upload to a google drive folder to establish the client path\n",
        "import requests\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "import pandas as pd\n",
        "\n",
        "# Keys and BQ table info\n",
        "FEC_API_KEY = \"DEMO_KEY\"  # Using your own key here is highly recommended\n",
        "COMMITTEE_ID = \"CXXXXXXXX\"  # As written, code only works with a campaign id specifically for FEC, NOT an individual's name and NOT for any other sites.\n",
        "BQ_PROJECT = \"project-name\"\n",
        "BQ_DATASET = \"dataset-name\"\n",
        "BQ_TABLE = \"table-name\"\n",
        "PER_PAGE = 100  # API limit\n",
        "# ___________________________\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# BigQuery client\n",
        "credentials = service_account.Credentials.from_service_account_file(\"/content/drive/MyDrive/folder/downloadedfromcreatingserviceacctkey.json\")\n",
        "bq_client = bigquery.Client(credentials=credentials, project=BQ_PROJECT)\n",
        "\n",
        "def fetch_fec_data(committee_id, api_key, per_page=100, max_pages=None):\n",
        "    \"\"\"All records from stated transaction period, including pagination for API call limits\"\"\"\n",
        "    url = \"https://api.open.fec.gov/v1/schedules/schedule_a/\"\n",
        "    page = 1\n",
        "    all_results = []\n",
        "\n",
        "    while True:\n",
        "        params = {\n",
        "            \"api_key\": api_key,\n",
        "            \"committee_id\": committee_id,\n",
        "            \"per_page\": per_page,\n",
        "            \"page\": page,\n",
        "            \"two_year_transaction_period\": 2024,  # one cycle\n",
        "            \"is_individual\": True\n",
        "        }\n",
        "        resp = requests.get(url, params=params)\n",
        "        if resp.status_code != 200:\n",
        "            raise Exception(f\"FEC API Error {resp.status_code}: {resp.text}\")\n",
        "\n",
        "        data = resp.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        if not results:\n",
        "            break\n",
        "\n",
        "        all_results.extend(results)\n",
        "\n",
        "        print(f\"âœ… Retrieved page {page} with {len(results)} records\")\n",
        "\n",
        "        # pagination check\n",
        "        if not data.get(\"pagination\") or data[\"pagination\"][\"pages\"] <= page:\n",
        "            break\n",
        "\n",
        "        page += 1\n",
        "        if max_pages and page > max_pages:\n",
        "            break\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "def normalize_records(records):\n",
        "    \"\"\"Map FEC fields into a clean schema for BigQuery\"\"\"\n",
        "    normalized = []\n",
        "    for r in records:\n",
        "        normalized.append({\n",
        "            \"contributor_name\": r.get(\"contributor_name\"),\n",
        "            \"contributor_state\": r.get(\"contributor_state\"),\n",
        "            \"contributor_employer\": r.get(\"contributor_employer\"),\n",
        "            \"contributor_occupation\": r.get(\"contributor_occupation\"),\n",
        "            \"contribution_receipt_date\": r.get(\"contribution_receipt_date\"),\n",
        "            \"contribution_amount\": r.get(\"contribution_receipt_amount\"),\n",
        "            \"committee_id\": r.get(\"committee_id\"),\n",
        "            \"report_year\": r.get(\"report_year\"),\n",
        "            \"transaction_id\": r.get(\"transaction_id\")\n",
        "        })\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def load_to_bigquery(records, project, dataset, table):\n",
        "    \"\"\"Load JSON records into BigQuery\"\"\"\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty. No data to load into BigQuery.\")\n",
        "        return\n",
        "\n",
        "    table_id = f\"{project}.{dataset}.{table}\"\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        write_disposition=\"WRITE_APPEND\",\n",
        "        autodetect=True\n",
        "    )\n",
        "\n",
        "    job = bq_client.load_table_from_dataframe(\n",
        "        df,\n",
        "        table_id,\n",
        "        job_config=job_config\n",
        "    )\n",
        "    job.result()\n",
        "\n",
        "    print(f\"ðŸš€ Loaded {len(df)} rows into {table_id}\")\n",
        "\n",
        "\n",
        "# ===== RUN PIPELINE =====\n",
        "fec_data = fetch_fec_data(COMMITTEE_ID, FEC_API_KEY, per_page=PER_PAGE)\n",
        "normalized = normalize_records(fec_data)\n",
        "load_to_bigquery(normalized, BQ_PROJECT, BQ_DATASET, BQ_TABLE)"
      ]
    }
  ]
}
